# Multimodal EEG and Speech Biomarker Extraction for MDD Detection

This repository presents an end-to-end multimodal machine learning framework for detecting **Major Depressive Disorder (MDD)** using **electroencephalography (EEG)** and **speech signals**. The project is built on the **MODMA (Multi-Modal Open Dataset for Mental Disorders Analysis)** dataset and integrates neurophysiological and behavioral biomarkers to improve diagnostic robustness and interpretability.

## ğŸ” Key Features
- Audio feature extraction using Librosa and Praat-based voice analysis
- EEG feature extraction from resting-state and ERP signals using MNE-Python
- Subject-level multimodal feature fusion
- Supervised classification using Logistic Regression and Random Forest
- Model serialization using joblib
- Interactive Streamlit dashboard for visualization and prediction

## ğŸ§  Motivation
Traditional depression diagnosis relies heavily on subjective assessments. This project aims to provide an **objective, data-driven, and explainable approach** by leveraging multimodal biosignals associated with depressive disorders.

## ğŸ“‚ Project Structure
- `notebooks/` â€“ Feature extraction and model training notebooks
- `data_processed/` â€“ Extracted and integrated feature datasets
- `Model_Output_Joblib/` â€“ Trained models and preprocessing pipelines
- `Streamlit/` â€“ Deployment dashboard and utilities

## ğŸš€ Applications
- Mental health research
- Clinical decision support systems
- Multimodal machine learning experimentation

## âš ï¸ Disclaimer
This project is intended for research purposes only and is not a substitute for professional medical diagnosis.
